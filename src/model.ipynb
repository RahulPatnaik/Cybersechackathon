{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9895331,"sourceType":"datasetVersion","datasetId":6077910},{"sourceId":9895336,"sourceType":"datasetVersion","datasetId":6077913}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:23:27.250671Z","iopub.execute_input":"2024-11-17T16:23:27.251263Z","iopub.status.idle":"2024-11-17T16:23:28.249817Z","shell.execute_reply.started":"2024-11-17T16:23:27.251204Z","shell.execute_reply":"2024-11-17T16:23:28.248904Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cyber-test/test (1).csv\n/kaggle/input/cyber-train/train (1).csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport pandas as pd\n\n# Load dataset\nfile_path = '/kaggle/input/cyber-train/train (1).csv'\ndata = pd.read_csv(file_path)\n\n# Preprocess the dataset\ndata = data.dropna(subset=['category', 'crimeaditionalinfo'])\nX = data['crimeaditionalinfo']\ny = data['category']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Initialize tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n\n# Preprocess data for Hugging Face\ndef preprocess_data(texts, labels, max_length=128):\n    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length)\n    inputs = torch.tensor(encodings['input_ids'])\n    masks = torch.tensor(encodings['attention_mask'])\n    labels = torch.tensor(labels)\n    return inputs, masks, labels\n\ntrain_inputs, train_masks, train_labels = preprocess_data(X_train, y_train)\ntest_inputs, test_masks, test_labels = preprocess_data(X_test, y_test)\n\n# Define a custom dataset class\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, masks, labels):\n        self.inputs = inputs\n        self.masks = masks\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.inputs[idx],\n            'attention_mask': self.masks[idx],\n            'labels': self.labels[idx]\n        }\n\ntrain_dataset = CustomDataset(train_inputs, train_masks, train_labels)\ntest_dataset = CustomDataset(test_inputs, test_masks, test_labels)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_steps=50\n)\n\n# Define Trainer\nfrom transformers import Trainer, EvalPrediction\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred: EvalPrediction):\n    preds = pred.predictions.argmax(-1)\n    labels = pred.label_ids\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)\n\n# Example Inference\ntext_example = [\"Describe a recent incident of cyber fraud.\"]\ninputs = tokenizer(text_example, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\noutputs = model(**inputs)\npredicted_class = torch.argmax(outputs.logits, dim=1).item()\npredicted_label = label_encoder.inverse_transform([predicted_class])[0]\nprint(\"Predicted Category:\", predicted_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:46:11.983654Z","iopub.execute_input":"2024-11-15T16:46:11.984011Z","iopub.status.idle":"2024-11-15T17:53:10.384473Z","shell.execute_reply.started":"2024-11-15T16:46:11.983978Z","shell.execute_reply":"2024-11-15T17:53:10.376785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)  # Move the model to the appropriate device\n\n# Example Inference\ntext_example = [\"bruh\"]\ninputs = tokenizer(text_example, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n\n# Move inputs to the same device as the model\ninputs = {key: value.to(device) for key, value in inputs.items()}\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient computation for inference\n    outputs = model(**inputs)\n\n# Get the predicted class\npredicted_class = torch.argmax(outputs.logits, dim=1).item()\npredicted_label = label_encoder.inverse_transform([predicted_class])[0]\nprint(\"Predicted Category:\", predicted_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:00:01.804278Z","iopub.execute_input":"2024-11-15T18:00:01.805171Z","iopub.status.idle":"2024-11-15T18:00:01.828027Z","shell.execute_reply.started":"2024-11-15T18:00:01.805130Z","shell.execute_reply":"2024-11-15T18:00:01.827189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch_xla transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:37:23.197074Z","iopub.execute_input":"2024-11-15T18:37:23.197787Z","iopub.status.idle":"2024-11-15T18:37:32.500932Z","shell.execute_reply.started":"2024-11-15T18:37:23.197752Z","shell.execute_reply":"2024-11-15T18:37:32.500046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install imbalanced-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:40:40.393233Z","iopub.execute_input":"2024-11-15T18:40:40.393653Z","iopub.status.idle":"2024-11-15T18:40:44.480267Z","shell.execute_reply.started":"2024-11-15T18:40:40.393618Z","shell.execute_reply":"2024-11-15T18:40:44.479089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tiktoken sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:41:55.761154Z","iopub.execute_input":"2024-11-15T18:41:55.761594Z","iopub.status.idle":"2024-11-15T18:42:00.059799Z","shell.execute_reply.started":"2024-11-15T18:41:55.761560Z","shell.execute_reply":"2024-11-15T18:42:00.058709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch_xla.core.xla_model as xm\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom imblearn.over_sampling import RandomOverSampler\nimport pandas as pd\n\n# Load dataset\nfile_path = '/kaggle/input/cyber-train/train (1).csv'\ndata = pd.read_csv(file_path)\n\n# Preprocess dataset\ndata = data.dropna(subset=['category', 'crimeaditionalinfo'])\nX = data['crimeaditionalinfo']\ny = data['category']\n\n# Handle rare classes\nclass_counts = y.value_counts()\nrare_classes = class_counts[class_counts < 2].index\ndata = data[~data['category'].isin(rare_classes)]\nX = data['crimeaditionalinfo']\ny = data['category']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Oversample to balance classes\noversampler = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = oversampler.fit_resample(pd.DataFrame(X), y_encoded)\n\n# Stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled.squeeze(), y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n)\n\n# Replace DeBERTa-v3 with RoBERTa-base\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=len(label_encoder.classes_)\n)\n\n# Move model to TPU\ndevice = xm.xla_device()\nmodel = model.to(device)\n\n# Preprocess data\ndef preprocess_data(texts, labels, max_length=512):\n    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length)\n    inputs = torch.tensor(encodings['input_ids'])\n    masks = torch.tensor(encodings['attention_mask'])\n    labels = torch.tensor(labels)\n    return inputs, masks, labels\n\ntrain_inputs, train_masks, train_labels = preprocess_data(X_train, y_train)\ntest_inputs, test_masks, test_labels = preprocess_data(X_test, y_test)\n\n# Custom dataset class\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, masks, labels):\n        self.inputs = inputs\n        self.masks = masks\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.inputs[idx],\n            'attention_mask': self.masks[idx],\n            'labels': self.labels[idx]\n        }\n\ntrain_dataset = CustomDataset(train_inputs, train_masks, train_labels)\ntest_dataset = CustomDataset(test_inputs, test_masks, test_labels)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    per_device_train_batch_size=16,  # TPU-compatible batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_steps=50,\n    warmup_steps=500,\n    report_to=\"none\",\n    tpu_num_cores=8  # TPU utilization\n)\n\n# Metrics for evaluation\ndef compute_metrics(eval_pred):\n    preds = eval_pred.predictions.argmax(axis=1)\n    labels = eval_pred.label_ids\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:43:39.814560Z","iopub.execute_input":"2024-11-15T18:43:39.815651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:25:12.303125Z","iopub.execute_input":"2024-11-17T16:25:12.303475Z","iopub.status.idle":"2024-11-17T16:25:25.259237Z","shell.execute_reply.started":"2024-11-17T16:25:12.303442Z","shell.execute_reply":"2024-11-17T16:25:25.258060Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom collections import Counter\n\n# Load dataset\nfile_path = '/kaggle/input/cyber-train/train (1).csv'  # Update this path if needed\ndata = pd.read_csv(file_path)\n\n# Preprocess the dataset\ndata = data.dropna(subset=['category', 'crimeaditionalinfo'])\ndata['cleaned_text'] = data['crimeaditionalinfo'].str.lower().str.replace(r'\\W', ' ')\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(data['category'])\n\n# Check class distribution\nclass_counts = Counter(y)\nprint(\"Initial class distribution:\")\nfor label, count in sorted(class_counts.items()):\n    print(f\"Class {label}: {count}\")\n\n# Remove classes with only one sample\nmin_samples = 5\nvalid_classes = [class_ for class_, count in class_counts.items() if count > min_samples]\nmask = np.isin(y, valid_classes)\nX_filtered = data['cleaned_text'][mask].reset_index(drop=True)\ny_filtered = y[mask]\n\n# Re-encode labels to ensure consecutive integers\nlabel_encoder_filtered = LabelEncoder()\ny_filtered = label_encoder_filtered.fit_transform(y_filtered)\n\nprint(f\"\\nNumber of classes after filtering: {len(label_encoder_filtered.classes_)}\")\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n)\n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create torch dataset\nclass CyberDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.encodings = tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create datasets\ntrain_dataset = CyberDataset(X_train, y_train, tokenizer)\ntest_dataset = CyberDataset(X_test, y_test, tokenizer)\n\n# Initialize model\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(label_encoder_filtered.classes_)\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    logging_dir='./logs',\n    logging_steps=100,\n    save_total_limit=2\n)\n\n# Define metrics computation\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\nprint(\"\\nStarting training...\")\ntrainer.train()\n\n# Evaluate the model\nprint(\"\\nEvaluating model...\")\neval_results = trainer.evaluate()\nprint(\"\\nEvaluation Results:\", eval_results)\n\n# Save the model and label encoder\nmodel.save_pretrained('./cyber_classifier_model')\ntokenizer.save_pretrained('./cyber_classifier_tokenizer')\nimport joblib\njoblib.dump(label_encoder_filtered, './label_encoder.joblib')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:42:02.456564Z","iopub.execute_input":"2024-11-17T16:42:02.456987Z","iopub.status.idle":"2024-11-17T17:37:15.584348Z","shell.execute_reply.started":"2024-11-17T16:42:02.456946Z","shell.execute_reply":"2024-11-17T17:37:15.583345Z"}},"outputs":[{"name":"stdout","text":"Initial class distribution:\nClass 0: 10877\nClass 1: 379\nClass 2: 480\nClass 3: 3608\nClass 4: 161\nClass 5: 1710\nClass 6: 183\nClass 7: 57416\nClass 8: 444\nClass 9: 12138\nClass 10: 56\nClass 11: 2822\nClass 12: 1\nClass 13: 1552\nClass 14: 1838\n\nNumber of classes after filtering: 14\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14052' max='14052' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14052/14052 49:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.680000</td>\n      <td>0.665983</td>\n      <td>0.765120</td>\n      <td>0.743041</td>\n      <td>0.730527</td>\n      <td>0.765120</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.582200</td>\n      <td>0.659882</td>\n      <td>0.776491</td>\n      <td>0.753294</td>\n      <td>0.753103</td>\n      <td>0.776491</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.581100</td>\n      <td>0.699082</td>\n      <td>0.773715</td>\n      <td>0.756031</td>\n      <td>0.748054</td>\n      <td>0.773715</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1171' max='1171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1171/1171 01:03]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results: {'eval_loss': 0.6990817189216614, 'eval_accuracy': 0.7737148347835371, 'eval_f1': 0.7560313696885839, 'eval_precision': 0.748053619165972, 'eval_recall': 0.7737148347835371, 'eval_runtime': 63.8586, 'eval_samples_per_second': 293.351, 'eval_steps_per_second': 18.337, 'epoch': 3.0}\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['./label_encoder.joblib']"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport joblib\n\ndef load_model_and_tokenizer(model_path='./cyber_classifier_model', \n                           tokenizer_path='./cyber_classifier_tokenizer',\n                           label_encoder_path='./label_encoder.joblib'):\n    \"\"\"\n    Load the saved model, tokenizer and label encoder\n    \"\"\"\n    # Load tokenizer and model\n    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n    model = BertForSequenceClassification.from_pretrained(model_path)\n    label_encoder = joblib.load(label_encoder_path)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    return model, tokenizer, label_encoder\n\ndef predict_category(text, model, tokenizer, label_encoder):\n    \"\"\"\n    Predict category for a given text\n    \"\"\"\n    # Ensure text is in list format\n    if isinstance(text, str):\n        text = [text]\n    \n    # Tokenize input\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding=True,\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n    \n    # Get prediction\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get predicted class\n    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n    \n    # Convert to label\n    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n    \n    # Get confidence scores\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n    confidence = probabilities[0][predicted_class].item()\n    \n    return {\n        'predicted_label': predicted_label,\n        'confidence': confidence,\n        'class_probabilities': probabilities[0].tolist()\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Load model, tokenizer, and label encoder\n    model, tokenizer, label_encoder = load_model_and_tokenizer()\n    \n    # Example texts\n    example_texts = [\n        \"Unauthorized access detected to company database through SQL injection\",\n        \"Received a suspicious email asking for bank details\",\n        \"DDoS attack targeting our web servers\"\n    ]\n    \n    # Make predictions\n    print(\"Making predictions...\")\n    for text in example_texts:\n        result = predict_category(text, model, tokenizer, label_encoder)\n        print(\"\\nText:\", text)\n        print(\"Predicted Category:\", result['predicted_label'])\n        print(f\"Confidence: {result['confidence']*100:.2f}%\")\n\n    # Single prediction example\n    single_text = \"Describe a recent incident of cyber fraud.\"\n    result = predict_category(single_text, model, tokenizer, label_encoder)\n    print(\"\\nSingle Prediction Example:\")\n    print(\"Text:\", single_text)\n    print(\"Predicted Category:\", result['predicted_label'])\n    print(f\"Confidence: {result['confidence']*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:37:46.432410Z","iopub.execute_input":"2024-11-17T17:37:46.433086Z","iopub.status.idle":"2024-11-17T17:37:46.882038Z","shell.execute_reply.started":"2024-11-17T17:37:46.433044Z","shell.execute_reply":"2024-11-17T17:37:46.881084Z"}},"outputs":[{"name":"stdout","text":"Making predictions...\n\nText: Unauthorized access detected to company database through SQL injection\nPredicted Category: 7\nConfidence: 95.10%\n\nText: Received a suspicious email asking for bank details\nPredicted Category: 7\nConfidence: 99.22%\n\nText: DDoS attack targeting our web servers\nPredicted Category: 5\nConfidence: 59.55%\n\nSingle Prediction Example:\nText: Describe a recent incident of cyber fraud.\nPredicted Category: 7\nConfidence: 97.77%\n","output_type":"stream"}],"execution_count":8}]}